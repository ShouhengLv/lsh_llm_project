import os
import faiss
import pickle
import numpy as np
import onnxruntime
from transformers import BertTokenizer  # æˆ– AutoTokenizer å–å†³äºæ¨¡å‹

# ========== å‚æ•°è®¾ç½® ==========
MODEL_PATH = "./models/text2vec/model.onnx"
TOKENIZER_PATH = "./models/text2vec"
DATA_PATH = "data/docs.txt"
INDEX_PATH = "rag/index.faiss"
CORPUS_PATH = "rag/corpus.pkl"
MAX_LINES = 100
BATCH_SIZE = 32

# ========== åˆå§‹åŒ– tokenizer å’Œ ONNX session ==========
tokenizer = BertTokenizer.from_pretrained(TOKENIZER_PATH)
session = onnxruntime.InferenceSession(MODEL_PATH, providers=["CPUExecutionProvider"])

# ========== è¯»å–å‰ MAX_LINES è¡Œæ–‡æœ¬ ==========
docs = []
with open(DATA_PATH, "r", encoding="utf-8") as f:
    for i, line in enumerate(f):
        if i >= MAX_LINES:
            break
        line = line.strip()
        if line:
            docs.append(line)

print(f"âœ… æˆåŠŸåŠ è½½æ–‡æœ¬æ•°é‡: {len(docs)}")

# ========== æ‰¹é‡ç¼–ç å‡½æ•° ==========
def encode(texts, batch_size=32):
    all_embeddings = []

    for i in range(0, len(texts), batch_size):
        batch = texts[i:i + batch_size]
        inputs = tokenizer(batch, return_tensors="np", padding=True, truncation=True, max_length=128)

        # è¡¥å…¨ token_type_idsï¼ˆæœ‰äº›æ¨¡å‹å¦‚ RoBERTa ä¸æä¾›ï¼‰
        if "token_type_ids" not in inputs:
            inputs["token_type_ids"] = np.zeros_like(inputs["input_ids"])

        ort_inputs = {
            "input_ids": inputs["input_ids"].astype(np.int64),
            "attention_mask": inputs["attention_mask"].astype(np.int64),
            "token_type_ids": inputs["token_type_ids"].astype(np.int64)
        }

        ort_outputs = session.run(None, ort_inputs)
        batch_embeddings = ort_outputs[0][:, 0, :]  # æå– CLS å‘é‡
        all_embeddings.append(batch_embeddings)

    return np.vstack(all_embeddings)

# ========== ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ ==========
print("ğŸš€ æ­£åœ¨ç¼–ç æ–‡æœ¬ä¸ºå‘é‡ ...")
embeddings = encode(docs, batch_size=BATCH_SIZE)
print(f"âœ… å‘é‡ç»´åº¦: {embeddings.shape}")

# ========== æ„å»ºå¹¶ä¿å­˜ FAISS ç´¢å¼• ==========
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(embeddings)

os.makedirs("rag", exist_ok=True)
faiss.write_index(index, INDEX_PATH)
with open(CORPUS_PATH, "wb") as f:
    pickle.dump(docs, f)

print("ğŸ‰ FAISS ç´¢å¼•æ„å»ºå®Œæˆï¼Œå·²ä¿å­˜åˆ° rag/ æ–‡ä»¶å¤¹ä¸‹")
